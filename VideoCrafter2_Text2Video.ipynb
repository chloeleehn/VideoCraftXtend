{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/VideoCrafter-colab/blob/main/VideoCrafter2_Text2Video_colab.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313139,"status":"ok","timestamp":1712792034515,"user":{"displayName":"Emm宥","userId":"02154495825231234225"},"user_tz":-480},"id":"CNvwy0Cv18wX","outputId":"1b6bccf2-10f0-4c8a-992d-d3af14b6b1f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1719,"status":"ok","timestamp":1712792287827,"user":{"displayName":"Emm宥","userId":"02154495825231234225"},"user_tz":-480},"id":"vAClybTp2HOm","outputId":"a8080381-a67e-4e4f-f009-4e34674f4e4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1XIExHgH8yTJSya4f1RGhW5Z-hwQwqif_/FYP/Code/VideoCrafter\n"]}],"source":["%cd /content/drive/MyDrive/FYP/Code/VideoCrafter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":941220,"status":"ok","timestamp":1712671531315,"user":{"displayName":"Emm宥","userId":"02154495825231234225"},"user_tz":-480},"id":"6W2VgRanJCUK","outputId":"a6523399-2a0e-4867-a5de-e70e19e3aee7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m617.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m940.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# install packages for VideoCrafer\n","!pip install -q decord einops omegaconf pytorch_lightning transformers av gradio==3.50.2 timm open_clip_torch kornia gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibCOXjPCmrri"},"outputs":[],"source":["# install packages for RIFE\n","%cd /content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE\n","!pip3 install -r requirements.txt\n","%cd /content/drive/MyDrive/FYP/Code/VideoCrafter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bC4FaN2G7QRB"},"outputs":[],"source":["!pip install vbench\n","!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jy9B5y2jqMhY"},"outputs":[],"source":["basedir = \"/content/drive/MyDrive/FYP/Code/VideoCrafter\"\n","# if you change the variables here, remember to change the \"name\" in .sh file\n","vidOut = \"results/cat\"\n","uvqOut = \"results/modified_prompts_cat\"\n","num_of_vid = 3\n","vid_length = 2\n","uvq_threshold = 3.8\n","fps = 24"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_rIhU83pDz1"},"outputs":[],"source":["# install packages for UVQ\n","# !pip3 install -r ./uvq/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44238,"status":"ok","timestamp":1712658408069,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"w8XE9VY8QXXe","outputId":"9c84e7fc-d247-412e-9bf8-3548d1a640ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["@CoLVDM Inference: 2024-04-09-10-26-10\n","Seed set to 123\n","AE working on z of shape (1, 4, 64, 64) = 16384 dimensions.\n",">>> model checkpoint loaded.\n","[rank:0] 1/1 samples loaded.\n","[rank:0] batch-1 (1)x1 ...\n","DDIM scale True\n","ddim device cuda:0\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/inference.py\", line 137, in <module>\n","    run_inference(args, gpu_num, rank)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/inference.py\", line 122, in run_inference\n","    batch_samples = batch_ddim_sampling(model, cond, noise_shape, args.n_samples, \\\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/funcs.py\", line 50, in batch_ddim_sampling\n","    samples, _ = ddim_sampler.sample(S=ddim_steps,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py\", line 114, in sample\n","    samples, intermediates = self.ddim_sampling(conditioning, size,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py\", line 193, in ddim_sampling\n","    outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py\", line 230, in p_sample_ddim\n","    e_t = self.model.apply_model(x, t, c, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/models/ddpm3d.py\", line 522, in apply_model\n","    x_recon = self.model(x_noisy, t, **cond, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/models/ddpm3d.py\", line 712, in forward\n","    out = self.diffusion_model(x, t, context=cc, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/networks/openaimodel3d.py\", line 556, in forward\n","    h = module(h, emb, context=context, batch_size=b)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/networks/openaimodel3d.py\", line 41, in forward\n","    x = layer(x, context)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/attention.py\", line 272, in forward\n","    x = block(x, context=context)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/attention.py\", line 214, in forward\n","    return checkpoint(self._forward, input_tuple, self.parameters(), self.checkpoint)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/common.py\", line 92, in checkpoint\n","    return ckpt(func, *inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24, in inner\n","    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 482, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 261, in forward\n","    outputs = run_function(*args)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/attention.py\", line 217, in _forward\n","    x = self.attn1(self.norm1(x), context=context if self.disable_self_attn else None, mask=mask) + x\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/evaluation/../../lvdm/modules/attention.py\", line 93, in forward\n","    sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.98 GiB is free. Process 239969 has 13.79 GiB memory in use. Of the allocated memory 10.46 GiB is allocated by PyTorch, and 2.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"]}],"source":["# Testing cell (do not modify)\n","!sh {basedir}/scripts/run_text2video.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7684,"status":"ok","timestamp":1712657429572,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"23QwbfryQy3t","outputId":"c56cc194-9538-40b9-a5ef-5b00269ab634"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE\n","Loaded v3.x HD model.\n","/content/drive/MyDrive/FYP/Code/VideoCrafter/results/test/000155204.mp4, 20.0 frames in total, 4.0FPS to 10FPS\n","Will not merge audio because using png or fps flag!\n"," 95% 19/20.0 [00:01<00:00,  9.81it/s]\n","/content/drive/MyDrive/FYP/Code/VideoCrafter\n"]}],"source":["# Testing cell (do not modify)\n","%cd /content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE\n","!python3 inference_video.py --exp=2 --video={basedir}/{vidOut}/000155204.mp4 --fps 10\n","%cd /content/drive/MyDrive/FYP/Code/VideoCrafter"]},{"cell_type":"markdown","metadata":{"id":"--1A8i4kKpfn"},"source":["### Clone VideoCrafter, setup env, and generate test videos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjYy0F2gZIPR"},"outputs":[],"source":["# !git clone -b v2.0 https://github.com/camenduru/VideoCrafter\n","\n","# !pip install -q decord einops omegaconf pytorch_lightning transformers av gradio==3.50.2 timm open_clip_torch kornia\n","# !pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n","\n","# !apt-get -y install -qq aria2\n","# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/VideoCrafter/VideoCrafter2/resolve/main/model.ckpt -d /content/drive/MyDrive/FYP/Code/VideoCrafter/checkpoints/base_1024_v1 -o model.ckpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281335,"status":"ok","timestamp":1708952217645,"user":{"displayName":"Emm宥","userId":"02154495825231234225"},"user_tz":-480},"id":"6CJ0Ys6iOaEw","outputId":"d9ea7436-bddc-4bb8-b578-8785bf7ceea6"},"outputs":[{"name":"stdout","output_type":"stream","text":["AE working on z of shape (1, 4, 64, 64) = 16384 dimensions.\n","^C\n"]}],"source":["# !python gradio_app.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555290,"status":"ok","timestamp":1710670788339,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"wLQEXQ7qIbmj","outputId":"ff85dcbe-b9d8-44ee-b4fe-06a9bb86950d"},"outputs":[{"name":"stdout","output_type":"stream","text":["@CoLVDM Inference: 2024-03-17-10-10-43\n","Seed set to 123\n","AE working on z of shape (1, 4, 64, 64) = 16384 dimensions.\n",">>> model checkpoint loaded.\n","[rank:0] 1/1 samples loaded.\n","[rank:0] batch-1 (1)x1 ...\n","DDIM scale True\n","ddim device cuda:0\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Saved in results/base_512_v2. Time used: 397.82 seconds\n"]}],"source":["# !sh scripts/run_text2video.sh"]},{"cell_type":"markdown","metadata":{"id":"KLj0mG90KWu0"},"source":["### Modify text prompts using GPT API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WR3zkT1s1x20"},"outputs":[],"source":["# call the GPT API here\n","import requests\n","import fileinput\n","\n","# file = \"/content/drive/MyDrive/FYP/Code/VideoCrafter/scripts/run_text2video.sh\"\n","# file = \"/content/drive/MyDrive/FYP/Code/VideoCrafter/prompts/test_prompts.txt\"\n","\n","def call_gpt_api(prompt, isSentence = False):\n","    api_key = \"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\"\n","\n","    response = requests.post(\n","      'https://api.openai.com/v1/chat/completions',\n","      headers = {\n","          'Content-Type': 'application/json',\n","          'Authorization': f'Bearer {api_key}'\n","      },\n","      json = {\n","          'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': prompt}],\n","          'model': 'gpt-3.5-turbo',\n","          # 'prompt': prompt,\n","          'temperature': 0.4,\n","          'max_tokens': 200\n","    })\n","    response_json = response.json()\n","    choices = response_json['choices']\n","    contents = [choice['message']['content'] for choice in choices]\n","    contents = [sentence for sublist in contents for sentence in sublist.split('\\n')]\n","    # Remove the leading number and dot from each sentence\n","    sentences = [content.lstrip('1234567890.- ') for content in contents]\n","    if len(sentences)>2 and isSentence:\n","      sentences = sentences[1:]\n","    return sentences\n"]},{"cell_type":"markdown","metadata":{"id":"AxPqnQwmKkl9"},"source":["### Clone UVQ, setup env, and test using YouTube UGC Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1585,"status":"ok","timestamp":1707502395179,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"umrdqDzD7Wmc","outputId":"9c6798d0-9c55-4439-a5d6-7f6fe2ec0680"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/FYP/Code/VideoCrafter\n"]}],"source":["# %cd /content/drive/MyDrive/FYP/Code/VideoCrafter\n","# /!git clone https://github.com/google/uvq.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4179,"status":"ok","timestamp":1710666242557,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"dzOzr-SzKf4_","outputId":"82ebfb2e-5b3d-4c43-c021-35b2a47ee92c"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 88.0M  100 88.0M    0     0  24.4M      0  0:00:03  0:00:03 --:--:-- 24.4M\n"]}],"source":["# !curl -o Gaming_1080P-0ce6_orig.mp4 https://storage.googleapis.com/ugc-dataset/vp9_compressed_videos/Gaming_1080P-0ce6_orig.mp4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_LqHEzSNWwu"},"outputs":[],"source":["# !python3 ./uvq/uvq_main.py --input_files=\"Gaming_1080P-0ce6_orig,20,Gaming_1080P-0ce6_orig.mp4\" --output_dir results --model_dir ./uvq/models"]},{"cell_type":"markdown","metadata":{"id":"2XzrqkFAKz5m"},"source":["### Select highest scored video using UVQ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCw41_GYNvoL"},"outputs":[],"source":["# Generate the scores in csv files\n","def genScore():\n","  for i in range(1, num_of_vid+1):\n","    fileindex= f\"{i:04d}\"\n","    !python3 ./uvq/uvq_main.py --input_files=\"{fileindex},2,{basedir}/{vidOut}/{fileindex}.mp4\" --output_dir {uvqOut} --model_dir ./uvq/models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHP5Dskvz6od"},"outputs":[],"source":["def getScore(filename):\n","  # MOS_score defines the output of the uvq score\n","  lines = str(filename).split('\\n')\n","  last_line = lines[-1]\n","  MOS_score = last_line.split(',')[-1]\n","  MOS_score = MOS_score[:-2]\n","\n","  return MOS_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWeSOyAcgFxN"},"outputs":[],"source":["# MOS_score defines the Mean Opinion Score of prediction, if the video's MOS exceeds the threshold then we directly use this video\n","def chooseBestVideo():\n","  MOS_score_high = 0\n","  preferred_output = \"\"\n","  chosen_idx = 0\n","\n","  for i in range(1, num_of_vid+1):\n","    '''We loop thru this current processed video'''\n","    filedir = f\"{i:04d}\"\n","    filename = f\"{i:04d}_uvq.csv\"\n","    MOS = !cat \"{basedir}/{uvqOut}/{filedir}/{filename}\"\n","\n","    MOS_score = getScore(MOS)\n","    print(\"Video Index:\", f\"{i:04d}\", \"Score:\", MOS_score)\n","\n","    #if the MOS_score is higher than the previous video, we choose this video as our preferred video output\n","    if float(MOS_score)>float(MOS_score_high) or float(MOS_score) > uvq_threshold:\n","      MOS_score_high = MOS_score\n","      preferred_output = filename\n","      chosen_idx = i\n","\n","    if float(MOS_score) > uvq_threshold:\n","      break\n","  return chosen_idx\n","  # print(MOS_score_high)\n","  # print(preferred_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Slhd5XBEW3Pi"},"outputs":[],"source":["# 1m14s for 3 videos\n","# genScore()\n","# print(\"finish generating scores\")\n","# index = chooseBestVideo()\n","# print(\"video with highest score:\", index)"]},{"cell_type":"markdown","metadata":{"id":"WJLO0unQ6Zu6"},"source":["### VBench Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6538,"status":"ok","timestamp":1712466634234,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"q7H92Ato9tQE","outputId":"6ba29fae-a1a3-4157-bc30-b6ed9fabe436"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'VBench'...\n","remote: Enumerating objects: 1727, done.\u001b[K\n","remote: Counting objects: 100% (220/220), done.\u001b[K\n","remote: Compressing objects: 100% (168/168), done.\u001b[K\n","remote: Total 1727 (delta 104), reused 141 (delta 49), pack-reused 1507\u001b[K\n","Receiving objects: 100% (1727/1727), 18.60 MiB | 12.75 MiB/s, done.\n","Resolving deltas: 100% (726/726), done.\n","Updating files: 100% (339/339), done.\n"]}],"source":["# !git clone https://github.com/Vchitect/VBench.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12279,"status":"ok","timestamp":1712511789264,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"3g5I9j2K7esw","outputId":"24ac9f37-6fbc-48fe-a155-1db3330292f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: Namespace(output_path='./evaluation_results/', full_json_dir='/content/drive/MyDrive/FYP/Code/VideoCrafter/VBench/vbench/VBench_full_info.json', videos_path='//content/drive/MyDrive/FYP/Code/VideoCrafter/results/modified_prompts/0003_4X_40fps.mp4', dimension=['motion_smoothness', 'imaging_quality'], load_ckpt_from_local=None, read_frame=None, custom_input=True)\n","start evaluation\n","Evaluation meta data saved to ./evaluation_results/results_2024-04-07-17:42:58_full_info.json\n","cur_full_info_path: ./evaluation_results/results_2024-04-07-17:42:58_full_info.json\n","Loading [networks.AMT-S.Model] from [/root/.cache/vbench/amt_model/amt-s.pth]...\n","100% 1/1 [00:02<00:00,  2.03s/it]\n","2024-04-07 17:43:04,825 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","cur_full_info_path: ./evaluation_results/results_2024-04-07-17:42:58_full_info.json\n","Loading pretrained model MUSIQ from /root/.cache/vbench/pyiqa_model/musiq_spaq_ckpt-358bb6af.pth\n","100% 1/1 [00:02<00:00,  2.35s/it]\n","Evaluation results saved to ./evaluation_results/results_2024-04-07-17:42:58_eval_results.json\n","done\n"]}],"source":["!python VBench/evaluate.py --dimension 'motion_smoothness' 'imaging_quality'  --videos_path /{basedir}/{vidOut}/0003_4X_40fps.mp4 --custom_input"]},{"cell_type":"markdown","metadata":{"id":"r0xAldETNh5S"},"source":["### RIFE Testing\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1927,"status":"ok","timestamp":1709832931927,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"osn6PDlJWGFp","outputId":"be142bbf-7ccc-4d96-ecdc-9042dd7a8206"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ECCV2022-RIFE'...\n","remote: Enumerating objects: 1987, done.\u001b[K\n","remote: Counting objects: 100% (269/269), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 1987 (delta 241), reused 209 (delta 206), pack-reused 1718\u001b[K\n","Receiving objects: 100% (1987/1987), 4.10 MiB | 13.62 MiB/s, done.\n","Resolving deltas: 100% (1263/1263), done.\n"]}],"source":["# !git clone https://github.com/megvii-research/ECCV2022-RIFE.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1711298075249,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"SCftzM7QRihM","outputId":"febee22c-f672-4f60-e7fa-5b1d8bc61684"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE\n"]}],"source":["# %cd /content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsriWI4Z_Ily"},"outputs":[],"source":["# !pip3 install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPL0w7OJO2Nb"},"outputs":[],"source":["# !python3 inference_video.py --exp=2 --video=/content/drive/MyDrive/FYP/Code/VideoCrafter/results/base_512_v2/0003.mp4"]},{"cell_type":"markdown","metadata":{"id":"K5R0zMsbTU6r"},"source":["### IFRNet Testing\n","Only intake 2 images?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3396,"status":"ok","timestamp":1710090539381,"user":{"displayName":"Hsin-ning LEE","userId":"07731733937501311439"},"user_tz":-480},"id":"OW2JrdrETXeO","outputId":"2402e937-106e-4140-a08b-2af44b0f1345"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'IFRNet'...\n","remote: Enumerating objects: 269, done.\u001b[K\n","remote: Counting objects: 100% (101/101), done.\u001b[K\n","remote: Compressing objects: 100% (67/67), done.\u001b[K\n","remote: Total 269 (delta 85), reused 36 (delta 34), pack-reused 168\u001b[K\n","Receiving objects: 100% (269/269), 16.71 MiB | 16.15 MiB/s, done.\n","Resolving deltas: 100% (111/111), done.\n"]}],"source":["# !git clone https://github.com/ltkong218/IFRNet.git"]},{"cell_type":"markdown","metadata":{"id":"HmaA44Yxefgr"},"source":["### Interpolation using RIFE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1voR6Lg7WIPw"},"outputs":[],"source":["def interpolation(chosen_idx, fps):\n","  vid_filename = f\"{chosen_idx:04d}.mp4\"\n","  %cd /content/drive/MyDrive/FYP/Code/VideoCrafter/ECCV2022-RIFE\n","  !python3 inference_video.py --exp=2 --video={basedir}/{vidOut}/{vid_filename}\n","  %cd /content/drive/MyDrive/FYP/Code/VideoCrafter\n","  #TODO here we do the caculation\n","  fps =\n","  out_name = f\"{chosen_idx:04d}_4X_{fps}fps.mp4\"\n","\n","  return out_name"]},{"cell_type":"markdown","metadata":{"id":"H1zP8F6pZoD_"},"source":["### Generate videos + Gradio Interface\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDcr_y_JZqmh"},"outputs":[],"source":["import gradio as gr\n","\n","def generate_output(input_text, output_video_1, fps):\n","    def generate_output_fn(input_text, output_video_1, fps):\n","        output = call_gpt_api(prompt = f\"Generate 2 similar prompts and add some reasonable words to the given prompt and not change the meaning, each within 30 words: {input_text}\", isSentence = True)\n","        output.append(input_text)\n","        with open(f\"{basedir}/prompts/test_prompts.txt\", 'w') as file:\n","          for i, sentence in enumerate(output):\n","            if i < len(output) - 1:\n","                file.write(sentence + '\\n')\n","            else:\n","                file.write(sentence)\n","        !sh {basedir}/scripts/run_text2video.sh\n","        #Connect the video output and return the video corresponding link\n","        genScore()\n","        chosen_idx = chooseBestVideo()\n","        chosen_vid_path = interpolation(chosen_idx, fps)\n","        chosen_vid_path = f\"{basedir}/{vidOut}/{chosen_vid_path}\"\n","        output_video_1 = gr.Video(value = chosen_vid_path)\n","\n","        #TODO update the t2v_examples prompt\n","        t2v_examples_modified = call_gpt_api(prompt = f\"Generate 5 similar prompts to the given prompt and not change the meaning, each within 10 words: {input_text}\" )\n","\n","        input_text = \"\"\n","        return input_text, output_video_1\n","    return generate_output_fn\n"]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"iBbXlMczNdZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atwHicx6ZQ-G"},"outputs":[],"source":["### for example\n","\n","import os\n","import sys\n","import gradio as gr\n","sys.path.insert(1, os.path.join(sys.path[0], 'lvdm'))\n","\n","t2v_examples = [\n","    ['A tiger walks in the forest, photorealistic, 4k, high definition'],\n","    ['an elephant is walking under the sea, 4K, high definition'],\n","    ['an astronaut riding a horse in outer space'],\n","    ['a monkey is playing a piano'],\n","    ['A fire is burning on a candle'],\n","    ['a horse is drinking in the river'],\n","    ['Robot dancing in times square'],\n","]\n","\n","\n","def t2v_demo(result_dir='./tmp/'):\n","\n","    with gr.Blocks(analytics_enabled=False) as videocrafter_iface:\n","        gr.Markdown(\"<div align='center'> <h2> T2V: an enhancement for a better quality </span> </h2> </div>\")\n","\n","        #######t2v#######\n","        with gr.Tab(label=\"Text2Video\"):\n","            with gr.Column():\n","                with gr.Row():\n","                    with gr.Column():\n","                        input_text = gr.Text(placeholder = t2v_examples[0], label='Prompts')\n","                        with gr.Row():\n","                          gr.Examples(examples=t2v_examples,\n","                            inputs=[input_text],\n","                            # fn=text2video.get_prompt,\n","                            cache_examples=False)\n","                        with gr.Row():\n","                            cfg_scale = gr.Slider(minimum=1.0, maximum=30.0, step=0.5, label='CFG Scale', value=12.0, elem_id=\"cfg_scale\")\n","                            fps = gr.Slider(minimum=4, maximum=32, step=1, label='fps', value=16, elem_id=\"fps\")\n","                        send_btn = gr.Button(\"Send\")\n","                    with gr.Column():\n","                      with gr.Tab(label='result'):\n","                          with gr.Row():\n","                            output_video_1 =  gr.Video(value= \"/content/drive/MyDrive/FYP/Code/VideoCrafter/results/modified_prompts/0003_4X_40fps.mp4\")\n","\n","            send_btn.click(\n","                fn = generate_output(input_text, output_video_1, fps),\n","                inputs=[input_text],\n","                outputs=[input_text, output_video_1],\n","            )\n","\n","    return videocrafter_iface\n","\n","if __name__ == \"__main__\":\n","    result_dir = os.path.join('./', 'results')\n","    t2v_iface = t2v_demo(result_dir)\n","    t2v_iface.queue(max_size=10)\n","    t2v_iface.launch(debug = True)\n","    # videocrafter_iface.launch(server_name='0.0.0.0', server_port=80)"]},{"cell_type":"markdown","metadata":{"id":"bZkbCrt0EXts"},"source":["### Evaluation using UVQ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e16sit9tbd-B"},"outputs":[],"source":["def evaluation(vid_path, outputDir):\n","  !python3 ./uvq/uvq_main.py --input_files=\"{outputDir},2,{vid_path}\" --output_dir {vidOut} --model_dir ./uvq/models\n","  filename = f'{outputDir}_uvq.csv'\n","  eval_csv = f\"{basedir}/{vidOut}/{outputDir}/{filename}\"\n","  score = getScore(eval_csv)\n","\n","  return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4T9wuMjmihjF"},"outputs":[],"source":["chosen_vid_path = \"0001_2X_20fps.mp4\"\n","path = f\"{basedir}/{vidOut}/{chosen_vid_path}\"\n","# print(path)\n","score = evaluation(path, \"after\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["--1A8i4kKpfn","AxPqnQwmKkl9"],"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}